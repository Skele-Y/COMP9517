{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6e41dd2-7519-4a2e-80f9-b0691195cc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm # for loading progress\n",
    "\n",
    "import seaborn as sns #  for plotting\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC,LinearSVC, LinearSVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../elpv-dataset/utils')\n",
    "from elpv_reader import load_dataset\n",
    "\n",
    "#from keras.applications.vgg19 import VGG19, preprocess_input\n",
    "#from keras.preprocessing import image\n",
    "#from keras.models import Model\n",
    "\n",
    "\"\"\"image_preprocessor = ImagePreprocessor()\n",
    "keypoint_detector = KeypointDetector() # SIFT_create()\n",
    "feature_descriptor = FeatureDescriptor() # sift : .compute\n",
    "encoder = VLAD_Encoder()\n",
    "classifier = Classifier()\n",
    "model = Model(keypoint_detector, feature_descriptor, encoder, classifier)\"\"\"\n",
    "\n",
    "\n",
    "class DataSplitter:\n",
    "    def __init__(self, encoder):\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def split(self, data):\n",
    "        data['binary_label'] = (data['label'] > 0).astype(int)\n",
    "        data['sample_weights'] = data['label'].apply(lambda x: 1 if (x == 0 or x == 1) else x)\n",
    "        #y_binary = np.where(y > 0, 1, 0)\n",
    "        #sample_weights = np.where((y == 0) | (y == 1), 1, y) \n",
    "        X_train, X_test, y_train, y_test, sw_train, sw_test = train_test_split(\n",
    "            data[['img_index', 'descriptors']],\n",
    "            data['binary_label'],\n",
    "            data['sample_weights'],\n",
    "            test_size=0.25,\n",
    "            random_state=42,\n",
    "            stratify=data['binary_label']#y_binary\n",
    "        )\n",
    "        X_train = self.encoder.encode(X_train['descriptors'])\n",
    "        X_test = self.encoder.encode(X_test['descriptors'])\n",
    "        return X_train, X_test, y_train, y_test, sw_train, sw_test\n",
    "\n",
    "\n",
    "class VLAD_Encoder:\n",
    "    def __init__(self, desc_dim, subset_fraction=0.25,  p=0.5, k=10, num_random_subsets=5):\n",
    "        \n",
    "        self.codebook = {}\n",
    "        self.desc_dim = desc_dim\n",
    "        self.num_random_subsets = num_random_subsets\n",
    "        self.p = p\n",
    "        self.m = num_random_subsets\n",
    "        self.subset_fraction = subset_fraction # 25% of training samples to be randomly sampled from\n",
    "        self.k = k # size codebook i.e. number of words\n",
    "        self.pca = PCA(whiten=True, n_components=128)\n",
    "\n",
    "    def _create_codebook(self, all_descriptors):\n",
    "        \"\"\"Requires flattened array.\"\"\"\n",
    "        num_descriptors = all_descriptors.shape[0]\n",
    "        subset_size = round(num_descriptors * self.subset_fraction)\n",
    "        \n",
    "        for i in range(self.num_random_subsets):\n",
    "            subset_indices = np.random.choice(num_descriptors, subset_size, replace=False)\n",
    "            subset_descriptors = all_descriptors[subset_indices]\n",
    "            kmeans = MiniBatchKMeans(n_clusters=self.k, random_state=i, batch_size=256*24) # 256*#cores\n",
    "            kmeans.fit(subset_descriptors)# replace with only test set samples\n",
    "            self.codebook[i] = kmeans\n",
    "\n",
    "    def _encode(self, descriptors):\n",
    "        vlad_vector = []#np.empty((1, m*k*d))\n",
    "        \n",
    "        for idx, kmeans_ in self.codebook.items():\n",
    "            cluster_assignments = kmeans_.predict(descriptors)\n",
    "            \n",
    "            vlad_vector_ = np.zeros((self.k, descriptors.shape[1]),) #(K, d)\n",
    "            \n",
    "            for idx, cluster_idx in enumerate(cluster_assignments):\n",
    "                vlad_vector_[cluster_idx] += (descriptors[idx] - kmeans_.cluster_centers_[cluster_idx])\n",
    "        \n",
    "            vlad_vector_ = np.sign(vlad_vector_) * np.abs(vlad_vector_) ** self.p\n",
    "            vlad_vector_ = normalize(vlad_vector_.reshape(1, -1), axis=1, norm='l2') # (1, Kd)\n",
    "            vlad_vector.append(vlad_vector_)\n",
    "        return np.hstack(vlad_vector)\n",
    "    \n",
    "    def encode(self, all_descriptors, descriptor_dim=128):\n",
    "        \"\"\"Takes in a DataFram column and return\"\"\"\n",
    "        fit_transform = False\n",
    "        \n",
    "        if not self.codebook:\n",
    "            # all_descriptors is grouped by image. Need to flatten it.\n",
    "            self._create_codebook(np.vstack(all_descriptors))\n",
    "            fit_transform = True\n",
    "\n",
    "        n_samples = len(all_descriptors)\n",
    "        all_vlad_vectors = np.empty((n_samples, self.m * self.k * self.desc_dim))\n",
    "        for idx, img_descriptors in enumerate(all_descriptors):\n",
    "            all_vlad_vectors[idx, :] = self._encode(img_descriptors)\n",
    "\n",
    "        if fit_transform:\n",
    "            all_vlad_vectors = self.pca.fit_transform(all_vlad_vectors)\n",
    "        else:\n",
    "            all_vlad_vectors = self.pca.transform(all_vlad_vectors)\n",
    "        return all_vlad_vectors\n",
    "        \n",
    "\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, descriptor_instance, detector_instance):\n",
    "        self.descriptor_instance = descriptor_instance\n",
    "        self.detector_instance = detector_instance\n",
    "\n",
    "    def compute(self, images):\n",
    "        \"\"\"\n",
    "            Returns:\n",
    "                all descriptors for each image    \n",
    "        \"\"\"\n",
    "\n",
    "        all_descriptors = [self._compute(img) for img in tqdm(images, desc=\"Computing feature desctriptors\")]\n",
    "        return all_descriptors\n",
    "\n",
    "    def _compute(self, image):\n",
    "        keypoints = self.detector_instance.detect(image)\n",
    "        \n",
    "        descriptors = self.descriptor_instance.compute(image, keypoints)[1]\n",
    "        return descriptors\n",
    "    \n",
    "\n",
    "class FeatureDescriptor:\n",
    "    def __init__(self, descriptor_instance):\n",
    "        self.descriptor_instance = descriptor_instance\n",
    "\n",
    "    def compute(self, images, keypoints):\n",
    "        \"\"\"\n",
    "            Returns:\n",
    "                all descriptors for each image    \n",
    "        \"\"\"\n",
    "        assert(len(images) == len(keypoints))\n",
    "        self.num_descriptors = 0\n",
    "        all_descriptors = [self._compute(img, kp) for img, kp in tqdm(zip(images, keypoints), desc=\"Computing feature desctriptors\")]\n",
    "        return all_descriptors\n",
    "\n",
    "    def _compute(self, image, keypoints):\n",
    "        descriptors = self.descriptor_instance.compute(image, keypoints)[1]\n",
    "        return descriptors\n",
    "        #self.num_descriptors += 1 if descriptors \n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, load_dataset, filepath):\n",
    "        self.load_dataset = load_dataset\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def load(self) -> pd.DataFrame:\n",
    "        \"\"\"Load the data and reduce it if required.\n",
    "\n",
    "            Returns:\n",
    "                pd.DataFram, columns = [img_index, type, proba]\n",
    "        \"\"\"\n",
    "        images, probas, types = self.load_dataset(self.filepath)\n",
    "        data, probas, types = self.load_dataset(*self.args,**self.kwargs)\n",
    "        img_ids = np.arange(images.shape[0])\n",
    "        df = pd.DataFrame({'img_index': img_ids, 'type': types, 'proba': probas})\n",
    "        reduced_df = reduce_dataset(df, 1.0) # reduce to 50% original\n",
    "        return images, reduced_df\n",
    "\n",
    "class ImagePreprocessor:\n",
    "    ...\n",
    "\n",
    "class KeypointDetector:\n",
    "    ...\n",
    "\n",
    "class DenseSampler:\n",
    "    def __init__(self, grid_size=60):\n",
    "        self.grid_size = grid_size\n",
    "\n",
    "    def detect(self, images: np.array) -> list:\n",
    "        \"\"\"Returns list of descriptors in order of images\"\"\"\n",
    "        descriptors = [self._detect(img) for img in tqdm(images, desc=\"Densley sampling image keypoints\")]\n",
    "        return descriptors\n",
    "    \n",
    "    def _detect(self, image):\n",
    "        assert image is not None\n",
    "        img_dim = image.shape[0]\n",
    "        n_cells_x = img_dim // self.grid_size\n",
    "        n_cells_y = img_dim // self.grid_size\n",
    "\n",
    "        # Calculate the centers of each grid cell as the keypoint\n",
    "        centers = tuple(cv.KeyPoint(x * self.grid_size + self.grid_size / 2, y * self.grid_size + self.grid_size / 2, self.grid_size)\n",
    "                       for y in range(n_cells_y) for x in range(n_cells_x))\n",
    "\n",
    "        return centers # cv.KeyPoint\n",
    "\n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self, clf, optimiser):\n",
    "        self.clf = clf\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.clf.fit(X)\n",
    "\n",
    "    def predict(self):\n",
    "        ...\n",
    "        \n",
    "\n",
    "class Model:\n",
    "    def __init__(self, keypoint_detector, feature_descriptor, encoder, clf):\n",
    "        self.keypoint_detector = keypoint_detector\n",
    "        self.feature_descriptor = feature_descriptor\n",
    "        self.encoder = encoder\n",
    "        self.clf = clf\n",
    "\n",
    "    def run(self):\n",
    "        keypoints = self.keypoint_detector.detect()\n",
    "        descriptors = self.feature_descriptor.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fd6b1ec-1b90-41bb-8d4d-1e2cd8fb965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, probas, types = load_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1fc9bf2-0a52-49f1-bdb2-12968b59fcb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keypoint_detector = DenseSampler(grid_size=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d025da29-04aa-45e7-93bd-aa73cec4c2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Densley sampling image keypoints: 100%|██████████| 2624/2624 [00:00<00:00, 100924.84it/s]\n"
     ]
    }
   ],
   "source": [
    "keypoints = keypoint_detector.detect(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95f24062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature desctriptors: 100%|██████████| 2624/2624 [00:22<00:00, 116.41it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "method = 'akaze'\n",
    "\n",
    "if method == 'sift':\n",
    "    sift = cv.SIFT_create()\n",
    "    feature_descriptor = FeatureDescriptor(sift)\n",
    "elif method == 'orb':\n",
    "    orb = cv.ORB_create()\n",
    "    feature_descriptor = FeatureDescriptor(orb)\n",
    "elif method == 'brisk':\n",
    "    brisk = cv.BRISK_create()\n",
    "    feature_descriptor = FeatureDescriptor(brisk)\n",
    "elif method == 'latch':\n",
    "    freak = cv.xfeatures2d.LATCH_create()\n",
    "    feature_descriptor = FeatureDescriptor(freak)\n",
    "elif method == 'brief':\n",
    "    brief = cv.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "    feature_descriptor = FeatureDescriptor(brief)\n",
    "elif method == 'akaze':\n",
    "    akaze = cv.AKAZE_create()\n",
    "    feature_descriptor = FeatureExtractor(akaze, akaze)\n",
    "\n",
    "    \n",
    "if method != 'akaze':\n",
    "    descriptors = feature_descriptor.compute(images, keypoints)\n",
    "else:\n",
    "    descriptors = feature_descriptor.compute(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90f6fcb7-b6df-4b10-85c3-7cb3e4832331",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_indices = np.arange(images.shape[0])\n",
    "data = pd.DataFrame({\n",
    "    'img_index': img_indices,\n",
    "    'descriptors': descriptors,\n",
    "    'label': probas,\n",
    "    'type': types\n",
    "})\n",
    "\n",
    "# Remove undetected keypoints\n",
    "data = data[data['descriptors'].apply(lambda d: d is not None and len(d) > 0)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a66d156d-878f-47d9-b09b-63e3427ffe86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shady\\anaconda3\\envs\\COMP9517\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 6144 or by setting the environment variable OMP_NUM_THREADS=4\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n",
      "c:\\Users\\shady\\anaconda3\\envs\\COMP9517\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 6144 or by setting the environment variable OMP_NUM_THREADS=4\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n",
      "c:\\Users\\shady\\anaconda3\\envs\\COMP9517\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 6144 or by setting the environment variable OMP_NUM_THREADS=4\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n",
      "c:\\Users\\shady\\anaconda3\\envs\\COMP9517\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 6144 or by setting the environment variable OMP_NUM_THREADS=4\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n",
      "c:\\Users\\shady\\anaconda3\\envs\\COMP9517\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 6144 or by setting the environment variable OMP_NUM_THREADS=4\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    }
   ],
   "source": [
    "descriptor_dim = feature_descriptor.descriptor_instance.descriptorSize()\n",
    "encoder = VLAD_Encoder(p=0.5, k=100, num_random_subsets=5, desc_dim=descriptor_dim)\n",
    "data_splitter = DataSplitter(encoder)\n",
    "X_train, X_test, y_train, y_test, sw_train, sw_test = data_splitter.split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4278f79a-ff71-4af3-b22d-c092583a0c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.90      0.81     315.0\n",
      "           1       0.69      0.40      0.50 172.33333333333337\n",
      "\n",
      "    accuracy                           0.72 487.33333333333337\n",
      "   macro avg       0.71      0.65      0.66 487.33333333333337\n",
      "weighted avg       0.72      0.72      0.70 487.33333333333337\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79     315.0\n",
      "           1       0.62      0.62      0.62 172.33333333333337\n",
      "\n",
      "    accuracy                           0.73 487.33333333333337\n",
      "   macro avg       0.71      0.71      0.71 487.33333333333337\n",
      "weighted avg       0.73      0.73      0.73 487.33333333333337\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shady\\anaconda3\\envs\\COMP9517\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    }
   ],
   "source": [
    "clfs = {'rf': RandomForestClassifier(), 'svm': LinearSVC(class_weight='balanced', C=1.0)}\n",
    "\n",
    "for clf_name, clf in clfs.items():\n",
    "    clf.fit(X_train, y_train, sample_weight=sw_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred, sample_weight=sw_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead892da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

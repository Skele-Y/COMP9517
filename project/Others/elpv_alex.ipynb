{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6e41dd2-7519-4a2e-80f9-b0691195cc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shady\\anaconda3\\envs\\COMP9517\\lib\\site-packages\\requests\\__init__.py:114: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (2.3.0)/charset_normalizer (None) doesn't match a supported version!\n",
      "  RequestsDependencyWarning,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm # for loading progress\n",
    "\n",
    "import seaborn as sns #  for plotting\n",
    "\n",
    "from sklearn.svm import SVC,LinearSVC, LinearSVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sys.path.append('../../elpv-dataset/utils')\n",
    "from elpv_reader import load_dataset\n",
    "\n",
    "from keras.applications.vgg19 import VGG19, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "\n",
    "\"\"\"image_preprocessor = ImagePreprocessor()\n",
    "keypoint_detector = KeypointDetector() # SIFT_create()\n",
    "feature_descriptor = FeatureDescriptor() # sift : .compute\n",
    "encoder = VLAD_Encoder()\n",
    "classifier = Classifier()\n",
    "model = Model(keypoint_detector, feature_descriptor, encoder, classifier)\"\"\"\n",
    "\n",
    "\n",
    "class DataSplitter:\n",
    "    def __init__(self, encoder):\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def split(self, data):\n",
    "        data['binary_label'] = (data['label'] > 0).astype(int)\n",
    "        data['sample_weights'] = data['label'].apply(lambda x: 1 if (x == 0 or x == 1) else x)\n",
    "        y_binary = np.where(y > 0, 1, 0)\n",
    "        sample_weights = np.where((y == 0) | (y == 1), 1, y) \n",
    "        X_train, X_test, y_train, y_test, sw_train, sw_test = train_test_split(\n",
    "            data[['img_index', 'descriptors']],\n",
    "            data['label'],\n",
    "            data['sample_weights'],\n",
    "            test_size=0.25,\n",
    "            random_state=42,\n",
    "            stratify=y_binary\n",
    "        )\n",
    "        X_train = self.encoder.encode(X_train['descriptors'])\n",
    "        X_test = self.encoder.encode(X_test['descriptors'])\n",
    "        return X_train, X_test, y_train, y_test, sw_train, sw_test\n",
    "\n",
    "\n",
    "class VLAD_Encoder:\n",
    "    def __init__(self,subset_fraction=0.25,  p=0.5, k=10, num_random_subsets=5):\n",
    "        \n",
    "        self.codebook = {}\n",
    "        self.num_random_subsets = num_random_subsets\n",
    "        self.p = p\n",
    "        self.m = num_random_subsets\n",
    "        self.subset_fraction = subset_fraction # 25% of training samples to be randomly sampled from\n",
    "        self.k = k # size codebook i.e. number of words\n",
    "        self.pca = PCA(whiten=True, n_components=128)\n",
    "\n",
    "    def _create_codebook(self, all_descriptors):\n",
    "        \"\"\"Requires flattened array.\"\"\"\n",
    "        num_descriptors = all_descriptors.shape[0]\n",
    "        subset_size = round(num_descriptors * self.subset_fraction)\n",
    "        \n",
    "        for i in range(self.num_random_subsets):\n",
    "            subset_indices = np.random.choice(num_descriptors, subset_size, replace=False)\n",
    "            subset_descriptors = all_descriptors[subset_indices]\n",
    "            kmeans = MiniBatchKMeans(n_clusters=self.k, random_state=i, batch_size=256*4) # 256*#cores\n",
    "            kmeans.fit(subset_descriptors)# replace with only test set samples\n",
    "            self.codebook[i] = kmeans\n",
    "\n",
    "    def _encode(self, kmeans_, descriptors):\n",
    "        vlad_vector = []#np.empty((1, m*k*d))\n",
    "        \n",
    "        for idx, kmeans_ in self.codebook.items():\n",
    "            cluster_assignments = kmeans_.predict(descriptors)\n",
    "            \n",
    "            vlad_vector_ = np.zeros((self.k, descriptors.shape[1]),) #(K, d)\n",
    "            \n",
    "            for idx, cluster_idx in enumerate(cluster_assignments):\n",
    "                vlad_vector_[cluster_idx] += (descriptors[idx] - kmeans_.cluster_centers_[cluster_idx])\n",
    "        \n",
    "            vlad_vector_ = np.sign(vlad_vector_) * np.abs(vlad_vector_) ** self.p\n",
    "            vlad_vector_ = normalize(vlad_vector_.reshape(1, -1), axis=1, norm='l2') # (1, Kd)\n",
    "            vlad_vector.append(vlad_vector_)\n",
    "        return np.hstack(vlad_vector)\n",
    "    \n",
    "    def encode(self, all_descriptors):\n",
    "        \"\"\"Takes in a DataFram column and return\"\"\"\n",
    "        fit_transform = False\n",
    "        \n",
    "        if not self.codebook:\n",
    "            # all_descriptors is grouped by image. Need to flatten it.\n",
    "            self._create_codebook(np.vstack(all_descriptors))\n",
    "            fit_transform = True\n",
    "\n",
    "        all_vlad_vectors = np.empty()\n",
    "        for idx, img_descriptors in enumerate(all_descriptors):\n",
    "            all_vlad_vectors[idx, :] = self._encode(img_descriptors)\n",
    "\n",
    "        if fit_transform:\n",
    "            all_vlad_vectors = self.pca.fit_transform(all_vlad_vectors)\n",
    "        else:\n",
    "            all_vlad_vectors = self.pca.transform(all_vlad_vectors)\n",
    "        return all_vlad_vectors\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "class FeatureDescriptor:\n",
    "    def __init__(self, descriptor_instance):\n",
    "        self.descriptor_instance = descriptor_instance\n",
    "\n",
    "    def compute(self, images, keypoints):\n",
    "        \"\"\"\n",
    "            Returns:\n",
    "                all descriptors for each image    \n",
    "        \"\"\"\n",
    "        assert(len(images) == len(keypoints))\n",
    "        self.num_descriptors = 0\n",
    "        all_descriptors = [self._compute(img, kp) for img, kp in zip(images, keypoints)]\n",
    "        return all_descriptors\n",
    "\n",
    "    def _compute(self, image, keypoints):\n",
    "        descriptors = self.descriptor_instance.compute(image, keypoints)[1]\n",
    "        #self.num_descriptors += 1 if descriptors \n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, load_dataset, filepath):\n",
    "        self.load_dataset = load_dataset\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def load(self) -> pd.DataFrame:\n",
    "        \"\"\"Load the data and reduce it if required.\n",
    "\n",
    "            Returns:\n",
    "                pd.DataFram, columns = [img_id, type, proba]\n",
    "        \"\"\"\n",
    "        images, probas, types = self.load_dataset(self.filepath)\n",
    "        data, probas, types = self.load_dataset(*self.args,**self.kwargs)\n",
    "        img_ids = np.arange(images.shape[0])\n",
    "        df = pd.DataFrame({'img_id': img_ids, 'type': types, 'proba': probas})\n",
    "        reduced_df = reduce_dataset(df, 1.0) # reduce to 50% original\n",
    "        return images, reduced_df\n",
    "\n",
    "class ImagePreprocessor:\n",
    "    ...\n",
    "\n",
    "class KeypointDetector:\n",
    "    ...\n",
    "\n",
    "class DenseSampler:\n",
    "    def __init__(self, grid_size=60):\n",
    "        self.grid_size = grid_size\n",
    "\n",
    "    def detect(self, images: np.array) -> list:\n",
    "        \"\"\"Returns list of descriptors in order of images\"\"\"\n",
    "        descriptors = [self._detect(images[i]) for i in tqdm(images, desc=\"Densley sampling image keypoints\")]\n",
    "        return descriptors\n",
    "    \n",
    "    def _detect(self, image):\n",
    "        assert image is not None\n",
    "        img_dim = image.shape[0]\n",
    "        print(img_dim)\n",
    "        n_cells_x = img_dim // self.grid_size\n",
    "        n_cells_y = img_dim // self.grid_size\n",
    "\n",
    "        # Calculate the centers of each grid cell as the keypoint\n",
    "        centers = tuple(cv.KeyPoint(x * self.grid_size + self.grid_size / 2, y * self.grid_size + self.grid_size / 2, self.grid_size)\n",
    "                       for y in range(n_cells_y) for x in range(n_cells_x))\n",
    "\n",
    "        return centers # cv.KeyPoint\n",
    "\n",
    "\n",
    "\"\"\"images, probas, types = load_dataset('./KOC/koc/data/labels.csv')\n",
    "keypoint_detector = DenseSampler(grid_size=60)\n",
    "keypoints = keypoint_detector.detect(images)\n",
    "\n",
    "sift = cv.SIFT_create()\n",
    "feature_descriptor = FeatureDescriptor(sift)\n",
    "descriptors = feature_descriptor.compute(images, keypoints) # List[np.ndarray(x, 128)]\n",
    "\n",
    "img_indices = np.arange(images.shape[0])\n",
    "data = pd.DataFrame({\n",
    "    'img_index': img_indices,\n",
    "    'descriptors': descriptors,\n",
    "    'label': probas,\n",
    "    'type': types\n",
    "})\n",
    "\n",
    "# Remove undetected keypoints\n",
    "data = data[data['descriptors'].apply(lambda d: d is not None and len(d) > 0)]\n",
    "\n",
    "encoder = VLAD_Encoder(p=0.5, k=100, num_random_subsets=5)\n",
    "#encoder.encode(feature_descriptor.num_descriptors)\n",
    "\n",
    "data_splitter = DataSplitter(encoder)\n",
    "X_train, X_test, y_train, y_test, sw_train, sw_test = data_splitter.split(data)\"\"\"\n",
    "\n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self, clf, optimiser):\n",
    "        self.clf = clf\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.clf.fit(X)\n",
    "\n",
    "    def predict(self):\n",
    "        ...\n",
    "\n",
    "#clfs = {'svm': LinearSVC, 'rf': RandomForestClassifier}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, keypoint_detector, feature_descriptor, encoder, clf):\n",
    "        self.keypoint_detector = keypoint_detector\n",
    "        self.feature_descriptor = feature_descriptor\n",
    "        self.encoder = encoder\n",
    "        self.clf = clf\n",
    "\n",
    "    def run(self):\n",
    "        keypoints = self.keypoint_detector.detect()\n",
    "        descriptors = self.feature_descriptor.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1fc9bf2-0a52-49f1-bdb2-12968b59fcb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "../../elpv-dataset/data/labels.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_40992\\102865556.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../../elpv-dataset/data/labels.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mkeypoint_detector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDenseSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\COMP9517labs\\elpv-dataset\\utils\\elpv_reader.py\u001b[0m in \u001b[0;36mload_dataset\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     data = np.genfromtxt(fname, dtype=['|S19', '<f8', '|S4'], names=[\n\u001b[1;32m---> 42\u001b[1;33m                          'path', 'probability', 'type'])\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[0mimage_fnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'probability'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\shady\\anaconda3\\envs\\COMP9517\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, like)\u001b[0m\n\u001b[0;32m   1791\u001b[0m             \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1792\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1793\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1794\u001b[0m             \u001b[0mfid_ctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1795\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\shady\\anaconda3\\envs\\COMP9517\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\shady\\anaconda3\\envs\\COMP9517\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    531\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[0;32m    532\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s not found.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: ../../elpv-dataset/data/labels.csv not found."
     ]
    }
   ],
   "source": [
    "images, probs, types = load_dataset()\n",
    "keypoint_detector = DenseSampler(grid_size=60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
